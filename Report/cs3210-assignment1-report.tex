\documentclass[a4paper,12pt]{article}


\usepackage{minted}
\usepackage[cm]{fullpage}
%\usepackage[a4paper,margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{capt-of}

\setminted[C]{fontsize=\small, tabsize=4, breaklines, linenos}
\setminted[Ruby]{fontsize=\small, tabsize=4, breaklines, linenos}
\setminted[text]{fontsize=\small, tabsize=4, breaklines, linenos}

%\setlength\parindent{0pt}

\title{Assignment 1 Report}
\author{Julius Putra Tanu Setiaji (A0149787E), Chen Shaowei (A0110560Y)}
\date{24 October 2018}

\begin{document}
\maketitle

\section{Program Design}
This train simulation is implemented in OpenMPI. Primary design considerations are:
\begin{itemize}
	\item Each edge is simulated by one process as required in the specifications.
	\item There is a master process responsible for the following:
	      \begin{itemize}
		      \item Distribution of information (map, lines) to slave processes.
		      \item Keeping track of train states (travelling/stationary).
		      \item Keeping track of station door open/close times.
		      \item Synchronising time across all slaves.
	      \end{itemize}
	\item Slave processes hold queues of trains and send them to each other.
\end{itemize}

\subsection*{Assumptions}

\begin{itemize}
	\item Only one train can open its doors at each at any one time, regardless of direction.
	\item Train stations have infinite capacity for waiting trains.
	\item Time units are discrete and can have no subdivisions
	      \begin{itemize}
		      \item \textbf{Implication}: It is sufficient to store all time units as integers instead of floating point numbers
	      \end{itemize}
	\item Trains must open their doors for at least 1 unit of time.
	      \begin{itemize}
		      \item \textbf{Implication}: We round every randomly generated door open time up to the nearest integer
	      \end{itemize}
\end{itemize}

\section{Points to Note / Implementation Details}
\begin{itemize}
	\item Current simulation time needs to be shared across all processes. In addition, time can only be advanced after all threads have completed the actions to be done in the current tick.
	      \begin{itemize}
		      \item \textbf{Implication}: Explicit synchronisation messages need to be sent between master and slaves before and after the advancement of time.
	      \end{itemize}
	\item Each slave process maintains two queues of trains. The first queue contains the trains that are waiting to enter the edge (\textit{entry queue}). The second queue contains the trains that are waiting (arrived and/or door open) at the next station but have yet to close their doors, and the time at which they would close their doors (\textit{exit queue}).
	      \begin{itemize}
		      \item For convenience, the same queue implementation is used for both queues. \textit{Entry queue}'s elements simply have garbage values for their second element.
	      \end{itemize}
	\item The logic for each slave process at each tick is as follows:
	      \begin{enumerate}
		      \item If a current train is occupying the edge and it can leave at the current time, the process will query master to find out when this train will finish closing its doors. The process will then add this train and the time received into its \textit{exit queue}. The edge will then be marked as available.
		      \item If there are trains waiting to access the edge, the process will dequeue the first one and set it as the current train.
		      \item If the slave process should spawn trains for any of the lines its edge falls on, it will do so, query master for their door closing time, and enqueue them (see step 1 above).
		      \item If the train at the head of \textit{exit queue} can close its doors at the \textbf{next} tick, the train is sent to the process containing the next edge that it should traverse.
		            \begin{itemize}
			            \item Since adjacent edges would only receive these trains at step 6, trains sent at this step would prematurely begin traversing the next edge.
			            \item This ensures that before step 2, every slave process already holds all trains that can begin traversing the edge (if the edge is unoccupied).
		            \end{itemize}
		      \item The slave process sends \textit{"no more trains"} messages to all adjacent edges.
		      \item The slave process waits for messages from all adjacent edges. If a train is sent, it is enqueued into the \textit{entry queue}. When \textit{"no more trains"} messages are received from all adjacent edges, this step is complete.
		      \item The slave process informs master that it has completed all actions for the current tick.
		      \item The slave process waits for an OpenMPI broadcast message that would advance time.
	      \end{enumerate}
	\item The logic for the master process at each tick is as follows:
	      \begin{enumerate}
		      \item The master process waits for messages from slave processes. If a request for next door close time is received, it computes the appropriate value, updates station statistics and sends it back. If it receives a message that all actions for the current tick have been completed, it increments a counter. When all slave processes have reported completion, this step is complete.
		      \item The master process prints the per-tick output, and broadcasts the next tick time.
		      \item If the simulation has reached completion, instead of broadcasting the next tick time, master will broadcast the shutdown signal.
	      \end{enumerate}
	\item The generic \mintinline{C}{MPI_Send} is used for slave-slave communication. Since messages are small, they are buffered and do not cause any deadlock. Deadlock does not occur when running all test cases in the lab. However, in the event that \mintinline{C}{MPI_Send} is not buffered and deadlock occurs, this can be resolved in the following ways:
	      \begin{itemize}
		      \item Non-blocking sends can be used for slave steps 4-5 and \mintinline{C}{MPI_Wait} called after slave step 6. This assumes that the non-blocking send will make process in the background even before \mintinline{C}{MPI_Wait} is called.
		      \item Alternatively, a window can be created via \mintinline{C}{MPI_Win_create}. All information to be communicated in slave steps 4-5 will be written to its own window. When all slave processes have finished updating their own window (synchronised with \mintinline{C}{MPI_Barrier}), \mintinline{C}{MPI_Get} calls can be used in slave step 6 to obtain the relevant information.
	      \end{itemize}
\end{itemize}

\section{Execution Time}

\subsection{Testcase Used}
We use a map generated by a Ruby script which can be found in appendix. The map generated will have at least 4 vertices with degree = 1, and all the paths forming the train lines are at least of length 4. However, these values are configurable.

We ran the same map for 100, 1000, and 10000 time-ticks with all possible different combinations of numbers of trains in each line as long as the total number of trains is between 1 and 64 inclusive. This results in around 47,900 testcases for each time-tick size. As such, we will only include the scatter diagram of the results in the report. However, should the need arise, csv files containing the raw data is attached together with the report.

Below you can find a sample input and visualisation of the adjacency matrix and the train lines. The parameters that we used are number of stations = 10, maximum distance between stations = 10.

Do also note that to facilitate more accurate execution time analysis, we disabled per-tick status output for the following tests.

% \begin{center}
% 	\captionof{figure}{Sample Input}
% 	\begin{minted}{text}
% 10
% Somerset,Tan Kah Kee,Redhill,Sembawang,Riviera,Samudera,Sengkang,Boon Lay,Tampines West,Tampines
% 0 0 0 0 0 0 0 0 6 0
% 0 0 1 0 0 0 1 0 4 1
% 0 1 0 0 6 0 0 0 0 0
% 0 0 0 0 0 0 0 4 0 9
% 0 0 6 0 0 0 0 0 0 0
% 0 0 0 0 0 0 0 0 0 10
% 0 1 0 0 0 0 0 0 0 0
% 0 0 0 4 0 0 0 0 0 0
% 6 4 0 0 0 0 0 0 0 0
% 0 1 0 9 0 10 0 0 0 0
% 0.1,0.2,0.1,0.1,1.0,0.5,0.7,0.8,0.5,0.5
% Somerset,Tampines West,Tan Kah Kee,Tampines,Sembawang,Boon Lay
% Samudera,Tampines,Tan Kah Kee,Sengkang
% Somerset,Tampines West,Tan Kah Kee,Sengkang
% 10000
% 21,22,21
% 		\end{minted}
% \end{center}

% \begin{center}
% 	\captionof{figure}{Map of the train line used, with the 3 lines indicated}
% 	\includegraphics[width=0.5\linewidth]{map}
% \end{center}

% \newpage
% \subsection{Graphs}
% Extreme outliers were removed from the scatter diagram.
% \begin{center}
% 	\captionof{figure}{Execution Time for 100 ticks}
% 	\centering
% 	\includegraphics[width=\linewidth]{100-time}
% \end{center}
% \begin{center}
% 	\captionof{figure}{Execution Time for 1,000 ticks}
% 	\centering
% 	\includegraphics[width=\linewidth]{1000-time}
% \end{center}
% \begin{center}
% 	\captionof{figure}{Execution Time for 10,000 ticks}
% 	\centering
% 	\includegraphics[width=\linewidth]{10000-time}
% \end{center}

\section{Discussion}
It can be observed that the wall clock time taken for the simulation to complete increases as number of threads increases. This is because as number of threads increases, there is more contention of resources -- in our case contention for each train (thread) in using links (edges) between stations (vertices) and contention for each train in opening door at each station. We are managing this using an implicit queue through implementing a timekeeper to track the next allowed event to occur, protected by marking that section as critical. As such, for each link (edge) or station (vertex), only one train (thread) is able to register itself to the timekeeper at any given time -- others will have to wait.

However, we have an interesting observation as well. Notice that the execution time behaves very differently when the number of threads is beyond the number of logical cores (20 cores). For small input size (100 ticks), when the number of threads is beyond the number of logical cores, the execution time actually falls. However, as input size gets larger (1,000 and 10,000 ticks), execution time increases. This can be explained that when the number of threads are below the number of logical cores, all the threads are running concurrently, resulting in more lock contention (not to be confused with resource contention). However, when number of threads is above the number of logical cores, the threads take turns to wake up and do work, resulting in less lock contention. For smaller input size, the lock contention time actually outweighs the execution time, resulting in the fall in execution time. However, for larger input size, there is a large overhead in context-switching which outweighs the effect of lock contention time. This is supported by the data we collected on number of context-switches for 100 ticks and 1,000 ticks.

We also observe another trend -- that the variance in execution time falls when the number of threads exceed the number of logical cores. We currently have no explanation on this, but we suspect that the compiler does an optimisation when the number of threads exceed the number of logical cores.

% \newpage
% \begin{center}
% 	\captionof{figure}{Number of context switches for 100 ticks}
% 	\includegraphics[width=0.9\linewidth]{100-cs}
% \end{center}
% \begin{center}
% 	\captionof{figure}{Number of context switches for 1,000 ticks}
% 	\includegraphics[width=0.9\linewidth]{1000-cs}
% \end{center}

\section{Bonus}
Starvation will never occur in the simulation program that we wrote. This is because to decide which train to open door or to be allowed to use a link next, we are using an implicit queue to implement First-Come-First-Serve (FCFS) scheduler. As such, every train is assured access to the link or permission to open door after a long enough time. The assumptions are that no train open its doors or travel using the links indefinitely (which we believe are fair to make).

\newpage
\section{Appendix A: Ruby script used to generate test cases}
Below is the code listing of the ruby script used to generate test cases. Essentially, it does:
\begin{enumerate}
	\item Create a random adjacency matrix with diagonal = 0
	\item Find the MST of the random graph created
	\item Ensure that there are enough vertices with degree = 1, else go back to step 1
	\item Enumerate the 2-combinations of the vertices with degree = 1, and pick 3 randomly.
	\item For each of the three 2-combinations, assign them to be the termini of each line.
	\item Using breadth-first-search, find the path between the two vertices for each pair of termini.
	\item Ensure that the path is long enough, else go back to step 4.
\end{enumerate}
\begin{minted}{Ruby}

  # SOME CODE

	\end{minted}
\end{document}
